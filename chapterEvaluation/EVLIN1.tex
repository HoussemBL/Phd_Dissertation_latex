\label{eval-evlin+}


We have performed a first user study to evaluate our first version of \prototype{} implementing the content-based query recommendation, the visualization recommendation and the quantification of recommendations.
{\color{Fuchsia}In what follows we use the name \prototypeOne{} to refer to the first version of \prototype{}  and to differentiate with our extended version of \prototype{} that will be discussed later in Section~\ref{sec:final-userstudy}.}




To evaluate \prototypeOne{}, we recruited 14 graduate students who are familiar with data analysis practices.
Our study began with a sample of a Formula one data warehouse exploration using \prototypeOne{}. After that, we gave students five minutes to train with the system.
Later, we asked seven students to explore using \prototypeOne{} the US domestic flights data warehouse for 25 minutes and to rate inspected recommendations using a 5-point scale from 1(not interesting) to 5 (highly interesting).
For each study, we collected the evolution provenance that tracks all exploration steps performed within  an exploration session as well as their ratings. 

Finally, at the end of each experiment, we ask participants to fill out an exit survey to evaluate the following points: (i) visualizations generated by \prototypeOne{}, (ii) recommended queries, (iii) benefit of impact matrix and (iv) usability of \prototypeOne{} using a 5-point scale. Additionally, we ask the participants to write explicitly their opinions to explain each evaluation.



%\paragraph*{Results.}
	  \begin{table}[b]
 \centering
 \scriptsize
 \resizebox{1\linewidth}{!}{
  \begin{tabu}{ |p{1.7cm}|p{1.7cm}| p{1.7cm}|p{1.7cm}| p{1.7cm}|p{1.7cm}| p{1.7cm}|}\hline
 avg\_rating($u_1$) & avg\_rating($u_2$) & avg\_rating($u_3)$ & avg\_rating($u_4$) & avg\_rating($u_5$) & avg\_rating($u_6$) & avg\_rating($u_7$) \\ \hline
 3.57& 2.27&3.22&3.69&3.42&3.5&3.5 \\ \hline
 \end{tabu}}
 %\vspace{-0.3em}
 \caption{Average ratings for each participant in the user study}
 \label{tab:avg-rating}
 \end{table}
Throughout our user study, participants performed seven exploration sessions containing around 140 exploration steps. We next describe our key findings and observations.


We have studied first the average of ratings made by each participant. Table~\ref{tab:avg-rating} depicts the results.
Overall, we see that most of the users have an average of ratings above three (except for user $u_2$). Recall that we are using a 5-point scale rating system where 1 signifies a not-interesting recommendation and 5 refers to an interesting recommendation. This indicates that the users' experiments were acceptable as most of the users have averages ratings above 3. This indicates that most of the participants succeed using \prototypeOne{} to reveal some interesting information when visually exploring the US domestic flights data warehouse.

%\begin{figure}[b]
%\centering
%\includegraphics[scale=0.5]{figures/userstudy1/ratings-dist.pdf}
%\caption{Ratings  breakdown in the user study}
%\label{fig:ratings-dist}
%\end{figure}
	
	
\begin{figure}[b]
\centering
\includegraphics[scale=0.55]{figures/userstudy1/ratings-prob.pdf}
		\caption{Average probability rating in the user study}
		\label{fig:prob-rating}
	\end{figure}

Next, we have studied the ratings made by all participants during the user study. 
%Figure~\ref{fig:ratings-dist} depicts the rating breakdown over the user study. It shows that 48\% of investigated exploration steps were highly rated (when rating equals to 4 or 5). This confirms our previous observation that users have surfaced interesting explorations during the user study.
%Yet, this breakdown of ratings may be influenced by a very successful exploration session where such user explores brilliantly the data and manages to select appropriately recommendations to analyze within the exploration session.
To this end, we compute the average of probabilities of each rating over performed exploration sessions. The results are depicted in Figure~\ref{fig:prob-rating}. This figure shows that ratings equal to 4 or 5 have an acceptable average of probability that is equal to 0.3 and 0.17 respectively.	This confirms our previous observation that users have surfaced interesting explorations during the user study.


	

%{\color{Fuchsia}
 We have also analyzed surveys filled by participants in the user study. We first asked participants about the visual recommendation feature. Results show that 57\% of participants find that the recommended visualizations are ``interesting'' and ``super interesting''. Indeed, most of them agree on the fact that the recommended visualizations are simple and easy to read.
Remaining users that are not satisfied with the generated visualizations provide various justifications. Some users found that some visualizations especially those encompassing three dimensions are hard to read and to compare. This concerns the case of the stacked bars and multi-line charts. Another intriguing observation from other participants not satisfied with the recommended visualizations was about the axis of charts that contain long labels that are long to read. While this is related to the content of the explored data warehouse, this observation can be taken into account in the future. 
%Indeed, we can simplify rendered information by using codes that refer to each depicted code. This visualization goes long with a glossary that explains used codes. \mel{not convincing argument} \hou{cleaning data could be a solution}
{\color{Fuchsia}Indeed, at more significant effort needs to be done in the pre-processing and the cleaning phase that precedes the visual exploration.}

Our survey contains also a question about the benefit of recommended queries. Overall, results show that around 85\% of participants are satisfied with the recommendations generated by \prototype{} and they found them interesting and helpful to reach important interpretations.
Indeed, we observe that many users were happy with extension recommendations and found them useful to expand user vision during the visual data exploration.  Examples of participants' replies in this context contain for instance ``Recommended queries of type extension were highly interesting and help me to broaden my knowledge and to find interesting correlations''. Other participants like the diversity of recommendation types. One participant wrote for instance ``Recommended queries of type extension were helpful to perform width exploration by adding other information to my current insight. Drill down and Zoom-in recommended queries help me to perform a depth exploration and to dig into details of my current exploration''.

Finally, we ask participants about the benefit of the impact matrix. Our analysis of surveys' replies reveals that around 57\% of participants trust the generated matrices and found them helpful to find interesting recommendations worth to explore next. We analyzed the replies of participants not satisfied with the impact matrix. We observe that it is sometimes hard to distinguish between the interestingness of recommendations using the impact matrix. In this context, we find many users' replies similar to the following statement ``I found many cells having red dark colors. To this end, I need to check all of them to verify which recommendation is really interesting to explore further''.
This observation was insightful and we use it subsequently in~\cite{Houssem:19:adbis,Houssem:19:IS} to provide further metrics susceptible to distinguish more clearly between recommendations' interestingness scores. 


%}


%~~\\	
%	metrics to study:
%\begin{itemize}
%\item survey evaluation in term of visualizations, matrix, query rec
%\item survey results sentences
%\end{itemize}

