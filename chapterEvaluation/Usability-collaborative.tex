\label{eva:usability}
%While the previous section focused on a quantitative performance analysis of our proposed collaborative recommendation methods, this section evaluates these methods qualitatively to assess their usefulness in supporting visual data exploration. 
While the previous section focused on a quantitative performance analysis of our proposed collaborative recommendation methods, this section evaluates the impact of introducing our collaborative-filtering recommendation method in the conduct of the visual data exploration.
We first validate in this section that the collaborative-filtering technique is effective in diversifying and improving the recommendations generated by our system \prototype{}. Subsequently, we assess in Section~\ref{sec:final-userstudy} users' satisfaction when visually exploring data warehouses based on a user study.


Similarly to the previous section, we use in this experiment the US flight data warehouse to evaluate the effectiveness of our collaborative-filtering query recommendation technique in diversifying and improving the recommendations generated by \prototype{}. This is explained by the fact that we need to collect initially a history of explorations targeting the same data warehouse to be able to perform our collaborative-filtering recommendation approach. 
Accordingly, we have recorded a set of exploration sessions targeting the US flight data warehouse.  



  \begin{figure}[b]
\centering
\includegraphics[scale=0.5]{figures/usability/KL-summary}
\caption{Accuracy study of various data recommendation configurations}
\label{fig:scores}
\end{figure}



Towards studying the impact of collaborative-filtering in diversifying recommendations, we study first the distribution of recommendation scores with (and without) collaborative recommendations.

To this end, we performed first a content-based recommendation for exploration steps specified in Table~\ref{tab:queries}. These recommendations are quantified subsequently using our \emph{recommendation quantifier} component (see Section~\ref{quantification-rec}). Recall that this latter implements Kullback Leibler (KL) to compute the deviation metric between recommendation's data region and the whole database as a utility score. 


Afterward, we compute the top-3 collaborative-filtering recommendations for the same exploration steps defined in Table~\ref{tab:queries} using our REC-hybrid approach. Recommendations generated by this approach are used to update interestingness scores computed initially using the Kullback Leibler (KL).

Interestingness scores computed with (or without) collaborative-filtering recommendation approach are then split into four ranges (i) [0-0.25] corresponds to scores that are less or equal to 0.25, (ii) ]0.25-0.5] corresponds to scores that are bigger than 0.25 and less or equal to 0.5; (iii) ]0.5-0.75] represents scores ranging between 0.5 and 0.75; and (iv) ]0.75-1] represent high interestingness scores larger than 0.75. 
Accordingly, we cluster interestingness scores computed with (or without) a collaborative-filtering recommendation approach by their corresponding ranges.
This leads to the generation of the horizontal mirror histogram depicted in Figure~\ref{fig:scores}.
This visualization compares the average distribution of the interestingness scores ranges of recommendations computed  for the exploration steps defined in Table~\ref{tab:queries} using the Kullback Leibler (KL), with (or without) collaborative-filtering recommendation approach. %




The analysis of the left side of this figure shows that all histograms associated with the quantification method, without collaborative-filtering encompass high density of interestingness scores whose values belong to the two ranges ]0.5-0.75] and ]0.75-1]. 
This indicates that adopting only the deviation metric (e.g., Kullback Leibler) to quantify recommendation is not sufficient as the user is left likely with a large number of highly interesting recommendations.

On the right hand side of Figure~\ref{fig:scores}, we observe that the blending deviation metric (computed using the Kullback Leibler) with the collaborative-filtering recommendations' information contributes to the decrease of interestingness scores whose values belong to the ranges ]0.5-0.75] and ]0.75-1].
This indicates that the collaborative-filtering technique succeeds to improve the distinction between recommendations having high scores and those having low scores.




%%%%%%%  Sorting experiments %%%%%%%%%%%%%%%


We show so far that the introduction of our collaborative-filtering recommendations approach leads to a significant change of interestingness scores distribution. 
In what follows, we study the impact of this change on the accuracy of quantifying recommendations.

To do that, we compare now the F1 score performance of (i) the quantification method implemented using Kullback Leibler (KL) and (ii) the mixed quantification that blends the interestingness scores computed using the Kullback Leibler function with those output by our collaborative-filtering recommendation approach.
 
%\mel{say why new rated dataset}
Similarly to the experiment described in Section~\ref{sec:effectivness-quant}, we first tracked 10 exploration sessions made by researchers in our department when exploring the flights data warehouse. In each exploration session, we invited our collaborators to rate the interestingness of five recommended queries using a 5-point scale from 1 (not interesting) to 5 (highly interesting).  
%{\color{Fuchsia}
Note that we didn't use already rated exploration sessions discussed in Section~\ref{sec:effectivness-quant}. This is due to the fact that sessions labeled in Section~\ref{sec:effectivness-quant} concern the three data warehouses presented in Section~\ref{evlin-ds} whereas the employment of collaborative-filtering in our current experiment requires only the flights data warehouse for it we maintain a multi-user graph.
%}
To avoid influencing researchers, we used during this experiment a specific version of \prototype{} that does not support the quantification of recommendations to avoid influencing participants during this experiment.


This results in a data set containing 50 rated recommendations. 
Afterward, we compute interestingness scores of these 50 recommendations using our two quantification approaches: (i) the first quantification method is based only on the Kullback Leibler (KL), and (ii) the second quantification method blends the interestingness scores computed using the Kullback Leibler function with the collaborative-filtering scores.
For the second approach, we compute top-3 collaborative recommendations using the REC-hybrid implementation applied on a multi-user graph containing 3300 nodes.
%of size equal to roughly 3300 nodes.
Subsequently, we leverage our collaborators' ratings to evaluate our two studied  approaches of recommendations' quantification in terms of F1 score. %\mel{again some verbal description wrt recall precision may be useful}
%(\hou{replace everywhere deviation based by Kullback}) 

To do that, we apply for each studied quantification approach the same process described in Section~\ref{sec:effectivness-quant}.
More specifically, we sort initially the 50 recommendations in descending order of their interestingness scores computed using each studied recommendations' quantification approach. Next, we compute  the F1 score (using the same formulas described in Section~\ref{sec:effectivness-quant}) associated with different sizes of the top-k recommendations.
We set initially $k$ to one and we compute F1 score. In the course of the experiment, $k$ is incremented iteratively (until reaching 50) and the F1 score is re-computed at each step.
To obtain the F1 score at each iteration, the F1 score of a quantification approach $f$, for a particular cut off setting $c$ ($\in [3,4,5]$), at ranking position $r$, we compute recall and precision metrics using the same formulas specified in Section~\ref{sec:effectivness-quant}.
%as follows:
%\begin{itemize}
%\item Recall is computed as $=\frac{TP_{cr}}{TP_{c}}$ with $TP_{c}$ is the total number of recommendations considered as interesting following users' ratings (user's rating is bigger or equal to $c$) and $TP_{cr}$ is the number of interesting recommendations obtained when considering the top-r recommendations sorted in descending order of their interestingness scores computed using the quantification function $f$. 
% 
% \item Precision is computed as $=\frac{\#TP_{cr}}{r}$ with $TP_{cr}$ is the number of interesting recommendations obtained when considering the top-r recommendations sorted in descending order of their interestingness scores computed using the quantification function $f$. The number $r$ is the ranking position. It indicates the total number of recommendations considered at this stage of computation. 
%
%\end{itemize} 
In a similar way to the experiment described in Section~\ref{sec:effectivness-quant}, we use in the course of this experiment three possible values of users' ratings ([3,4,5]) as possible cut-off values to define the set of recommendations deemed as interesting.
Accordingly, we repeat the process of F1 value computation for varying sizes of top-k recommendations and for each possible value of users' ratings ([3,4,5]). This leads to the computation of f-measure-3, f-measure-4, and f-measure-5 that correspond to F1 scores computed when setting the cut off value respectively to 3, 4, and 5.



 \begin{figure*}
        %\centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[scale=0.35]{figures/usability/KL-use-RP.pdf}
     \caption[Quantification process without the collaborative-filtering recommendation]
      {{\small Quantification process without the collaborative-filtering recommendation}}   
            \label{fig:KLdist}
        \end{subfigure}
       % \hfill
       \hspace{2.5em}
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
          \includegraphics[scale=0.35]{figures/usability/COLLAB-use-RP.pdf}   
        \caption[Quantification process  with the collaborative-filtering recommendation]
      {{\small Quantification process with the collaborative-filtering recommendation}}    
            \label{fig:EU-dist}
        \end{subfigure}

        \caption[Comparison of performances between the two quantification methods]
       {\small Comparison of performances between the two quantification methods} 
        \label{fig:scores-use-RP}
    \end{figure*}




Figure~\ref{fig:scores-use-RP} depicts the evolution of the F1 scores' values of our two studied recommendations quantification processes for various values of the cut-off.
We observe in this figure that the best score values of the F1 score are always obtained using the mixed quantification approach regardless of the value of the cut off.
More specifically, the best F1 score value reaches 0.84, 0.77, and 0.46 using the mixed quantification approach when setting the cut off to 3, 4 and 5 respectively while best values of F1 scores associated with the quantification approach based only on Kullback Leibler (KL) reach 0.75, 0.65 and 0.4 when setting the cut off to 3, 4 and 5 respectively.
Note also that the best F1 scores recorded for the mixed quantification approach are obtained earlier compared with the quantification approach implementing only the Kullback Leibler (KL).
For instance, when setting the cut off to 4, the best F1 score of the mixed quantification approach is obtained when considering the top-23 recommendations while we need to investigate the top-28 recommendations sorted in descending order using scores output by the quantification approach implementing only Kullback Leibler (KL). This same observation holds when setting the cut off to 5. Indeed, we need to investigate only the top-6 recommendations of the mixed quantification approach to reach the best F1 score performance whereas the best F1 score associated with the Kullback Leibler quantification approach is obtained when reaching the top-18 recommendations. 



Overall, we conclude from this experiment that taking into account the collaborative-filtering recommendations when quantifying recommendations contributes to an improvement of the F1 score. This leads thereby to the improvement of accuracy of quantification of recommendation process and indicates that we have now a better distinction between interesting and non-interesting recommendations. 
 Based on this observation, we expect a more effective visual data exploration experience when introducing the collaborative-filtering recommendation in our system \prototype{}. To this end, we investigate the impact of  extending our system \prototype{} with the collaborative-filtering recommendation in the users' experience when exploring visually data warehouses.










