\label{eva:merge}

We discussed in Section~\ref{collaborative-query-rec} our collaborative-filtering query recommendation approach. This latter relies on a multi-user graph obtained by merging periodically the evolution provenance graphs of users' exploration sessions.
In this section, we evaluate the performance of our merge approaches (proposed in Section~\ref{sec:fuse}) in terms of runtime, quality, and conciseness.


To do that, we use the US flight data warehouse to evaluate the performance of all proposed merge approaches.
 This is explained by the fact that the evaluation of proposed merge methods focuses mainly on the exploration session graphs' aspects (e.g., size, connectivity, etc$\ldots$) rather than the content of the data warehouse. Accordingly, we used the set of exploration sessions recorded initially when exploring the US flight data warehouse to understand the behavior of our proposed merge methods.
 
Furthermore, we implemented an exploration session generator that takes as input a set of real exploration sessions and generates variants for each seed. This generator is used later to evaluate the performance of our proposed merge methods.





Our first set of experiments studies the performance of three merging techniques described in Section~\ref{sec:finding}. Table~\ref{tab:merge-algo} summarizes the different merging techniques. For varying similarity threshold $\thetaFuse$ (set to 0.5, 0.7, and 0.9), we study the merge algorithms' runtimes and their fusion rates ($\frac{size\_multi-user\_graph }{\sum size\_users\_exploration\_sessions}$) on 5 exploration session workloads. Each workload contains a set of exploration sessions that comprise around 1000 exploration steps. Individual exploration sessions have sizes between 10 and 20 exploration steps. All algorithms iterate over the exploration sessions of each workload, incrementally merging each session into the multi-user graph.
 Figure~\ref{fig:threshold} reports results as averages over the five randomly generated workloads. 


 \begin{table}[t]
\centering
\scriptsize
\begin{tabular}{|c|l|} \hline
Name & Description \\ \hline
\rlm{} & The \rlmLong{} algorithm described in Algorithm~\ref{algo:rlm_main}\\  \hline
\mlm{}-CA & The \mlmLong{} algorithm (Algorithm~\ref{algo:mlm}) that \\
&avoids conflicts in the incrementally created matching \\ \hline
\mlm{}-CR & The \mlmLong{} algorithm that resolves conflicts,\\
& taking the matching with the highest similarity score \\ \hline
\end{tabular}
\caption{Merge-algorithms used in the provenance aggregator evaluation} 
\label{tab:merge-algo}
\end{table}



\begin{figure}[b]
\centering
\includegraphics[width=0.7\linewidth]{figures/merge/fusion_evaluation.pdf}
\vspace{-0.5em}
\caption{$\thetaFuse$ vs. merge rate (solid lines) and runtime (dashed lines)}
\label{fig:threshold}
\end{figure}



Considering the runtime results (secondary y-axis) presented as dashed lines in Figure~\ref{fig:threshold}, we first observe that \rlm{} is the fastest algorithm. 
For $\thetaFuse = 0.5$, both the runtime and merge rate significantly increase compared to more strict thresholds. This is explained by the relaxed threshold similarity value that matches most of the queries to one another.
The runtime of both \mlm{} variants is significantly higher than for \rlm{}, due to the larger search space when looking for matches. Yet, this benefits the merge rate, which indicates that the \mlm{} algorithms are more effective in aggregating exploration sessions when operating on moderate to high similarity thresholds. 
%For $\thetaFuse = 0.5$, we see that \rlm{} outperforms other merge methods in terms of merge rate. 
%This can be explained by the relaxed threshold similarity value that matches most of the queries to one another. Therefore, roots are matched with high probability in \rlm{}, which then triggers the recursive search of matches in subsequent layers.

Comparing \mlm{}-CA and \mlm{}-CR, we observe that \mlm-CR takes slightly more time, which is explained by a low rate of revoked matches. 
Consistently taking the best match as a starting point for a layered traversal also increases chances of finding matches during this traversal in cases where not a significant number of nodes matches overall (i.e., for thresholds 0.7 and 0.9). 

Overall, this experiment shows that \rlm{} outperforms the other studied  merge methods in terms of merge rate for $\thetaFuse = 0.5$, while the \mlm{} algorithms are more effective when adopting high values of $\thetaFuse$. 
Thereby, the assignment of the similarity threshold value impacts significantly the choice of the merge algorithm to use. 
Yet, setting the similarity threshold value is not an obvious task as it may impact the merge quality. 

Accordingly, we study the impact of the threshold similarity values (used in the previous experiment) on the merge quality. 
This is done by studying the evaluation metrics recall and precision. %adopted in Information Retrieval.



To do that, we prepared first, eight pairs of exploration session graphs input to merge. 
These eight pairs of graphs are synthetic exploration sessions generated using our exploration session generator based on real sessions collected when exploring the flights data warehouse.  The set of exploration graphs present in these eight pairs vary significantly in size to avoid the impact of this metric on the performance of our studied merge approaches.
For each graphs pair ($G_1$,$G_2$), we constructed $\matching_{complete}$ the full list of possible one-to-one matchings between nodes $n_1 \in G_1$ and  $n_2 \in G_2$.
Subsequently, we investigated manually the set of pair nodes available in $\matching_{complete}$ to construct $\matching_{valid}$, the subset of matching candidates worth performing. 
Note that $\matching_{valid}$ may contain overlapping matchings e.g., $\{(n_1,n_2), (n_1,n'_2)\} \subseteq \matching_{valid}$. This signifies that we have several alternatives to merge the node $n_1$. Recall that our merge algorithms \rlm{}, \mlm{}-CA and \mlm{}-CR perform 1:1 matching when merging two input graphs. Accordingly, we cluster $\matching_{valid}$, e.g., $\{(n_1,[n_2,n'_2])\} \in \matching_{valid}$. Consequently, we consider that such merge $\matching=(n_1,n_2)$ (output by one of our merge method) is sufficient to consider  $\{(n_1,[n_2,n'_2])\} \in \matching_{valid}$ as a fulfilled valid matching. 

Afterward, we merged each graphs pair using our three merge algorithms. For each merge approach, we tried various similarity threshold $\thetaFuse$ values (set to 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95 and 1). At each merge experiment, we collected the set of performed matchings $\matching_{performed}$. Those latter are compared to the $\matching_{valid}$ set in order to compute recall and precision metrics. 
More specifically, recall is equal to $\frac{|\matching_{performed} \cap \matching_{valid}|}{|\matching_{valid}|}$ while precision is equal to $\frac{|\matching_{performed} \cap \matching_{valid}|}{|\matching_{performed}|}$.



  \begin{table}[t]
 \centering
 \scriptsize
 \resizebox{0.8\linewidth}{!}{
  \begin{tabu}{|p{1.7cm}|p{1.7cm}| p{1.7cm}|p{1.7cm}| p{1.7cm}|}\hline
%Fuse threshold  &\multicolumn{3}{| c |}{Flights}  & \multicolumn{2}{| c |}{Airport}&     \\ 
\textbf{Similarity threshold $\thetaFuse$} &  \textbf{Metric} &\textbf{RLM}&\textbf{MLM-CA}&\textbf{MLM-CR}\\ \hline
 &  recall &0.45&0.6&0.66\\ 
\textbf{0.5} &  precision &0.37&0.5&0.53\\ 
 &  F1 &0.41&0.55&0.59\\ \hline
 &  recall &0.45&0.6&0.64\\ 
\textbf{0.6} &  precision &0.4&0.52&0.54\\ 
 &  F1 &0.42&0.55&0.58\\ \hline
 &  recall &0.43&0.64&0.73\\ 
\textbf{0.7} &  precision &0.4&0.61&0.66\\ 
 &  F1 &0.42&0.62&0.7\\ \hline
 &  recall &0.32&0.57&0.68\\ 
\textbf{0.8} &  precision &0.38&0.71&0.81\\ 
 &  F1 &0.35&0.63&0.74\\ \hline
 &  recall &0.32&0.59&0.68\\ 
\textbf{0.85} &  precision &0.44&0.94&0.94\\ 
 &  F1 &0.37&0.73&0.79\\ \hline
 &  recall &0.32&0.59&0.68\\ 
\textbf{0.9} &  precision &0.44&0.92&0.97\\ 
 &  F1 &0.37&0.72&0.8\\ \hline
 &  recall &0.29&0.46&0.56\\ 
\textbf{0.95} &  precision &0.5&1&1\\ 
&  F1 &0.36&0.63&0.71\\ \hline
 &  recall &0.29&0.44&0.53\\ 
\textbf{1} &  precision &0.5&1&1\\ 
 &  F1 &0.36&0.61&0.69\\ \hline
 \end{tabu}}
\caption{Study of merge quality with varying thresholds and varying merge approaches}
  \label{tab:merge-quality}
 \end{table}
 
%\begin{figure}
%  \centering
%     \begin{subfigure}{.4\textwidth}
% \includegraphics[width=1\textwidth]{figures/merge/thresh50}
%  \caption{Merge quality for $\thetaFuse = 0.5$}
%  \label{fig:thresh50}
%    \end{subfigure}
%      \hspace{1cm}
%    \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh60}
%   \caption{Merge quality for $\thetaFuse = 0.6$}
%    \label{fig:thresh60}
% \end{subfigure} 
%  \vspace{0.7cm}
% \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh70}
%      \caption{Merge quality  for $\thetaFuse = 0.7$}
%        \label{fig:thresh70}
%    \end{subfigure}
%  \hspace{1cm}
%   \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh80}
%      \caption{Merge quality  for $\thetaFuse = 0.8$}
%      \label{fig:thresh80}
%    \end{subfigure}
% 
%   \vspace{0.7cm}   
%         \begin{subfigure}{.4\textwidth}
% \includegraphics[width=1\textwidth]{figures/merge/thresh85}
%  \caption{Merge quality  for $\thetaFuse = 0.85$}
%  \label{fig:thresh85}
%    \end{subfigure}
%      \hspace{1cm}
%    \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh90}
%   \caption{Merge quality  for $\thetaFuse = 0.9$}
%    \label{fig:thresh90}
% \end{subfigure} 
%
%    
%  \vspace{0.7cm}
% \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh95}
%      \caption{Merge quality for $\thetaFuse = 0.95$}
%        \label{fig:thresh95}
%    \end{subfigure}
%  \hspace{1cm}
%   \begin{subfigure}{.4\textwidth}
%  \includegraphics[width=1\textwidth]{figures/merge/thresh100}
%      \caption{Merge quality for $\thetaFuse = 1$}
%      \label{fig:thresh100}
%    \end{subfigure}
%
%  \caption{Study of merge quality with varying thresholds and varying merge approaches}
%  \label{fig:merge-quality}
%\end{figure}





%Figure~\ref{fig:merge-quality}
Table~\ref{tab:merge-quality} reports for different values of $\thetaFuse$, the recall, precision and F1 results as averages over the eight graph pairs merged using different merge approaches.


%Over the eight results depicted in Figure~\ref{fig:merge-quality}, \
Over results available in Table~\ref{tab:merge-quality}, we observe first that \rlm{} has always mediocre performances in terms of recall, precision, and F1 scores regardless of the value of $\thetaFuse$. More specifically, performance scores associated with \rlm{} never reach a value above 0.5.




Contrarily, we see that \mlm{} algorithms have better results. Thus, the \mlm{} variants have highly similar performances with slightly better performance for \mlm{}-CR.
Overall, \mlm{} merge family methods outperform \rlm{} in the recall, precision, and, F1 scores regardless of the value of $\thetaFuse$. 
Consequently, the use of \mlm{} algorithms is more appropriate to obtain a good quality of merge.

Our final observation is that the \mlm{}-CR method outperforms consistently the \mlm{}-CA merge method in all the studied quality metrics regardless of the value of $\thetaFuse$. 
This is explained by the conflicting matching candidates' policy adopted by the two methods.
Hence, \mlm{}-CA prioritizes the first discovered match, even if its distance is higher than the alternative matching candidate. On the other side, \mlm{}-CR performs all matching proposals encompassing the lowest distance. This increases the likelihood of subsequent application of recursive merge between two highly similar sub-graphs.



Overall, Table~\ref{tab:merge-quality}  shows that \mlm{}-CR outperforms the other studied merge methods in terms of quality regardless of the similarity threshold value.
This is clear when visualizing Figure~\ref{fig:merge-rp} that summarizes the F1 score's value evolution of the three studied merge methods for varying values of the similarity threshold.

\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{figures/merge/merge-RP}
 \caption{Study of threshold values impact on quality merge methods}
 \label{fig:merge-rp}
\end{figure}

Another intriguing observation from Figure~\ref{fig:merge-rp} is related to setting the similarity threshold.
Indeed, this figure shows that the range of interest related to the setting of the similarity threshold is between 0.7 and 0.95 values (where we got good performance of MLM merge family methods).
For this particular setting of experiment (where we explore only the flights data warehouse) we see in Figure~\ref{fig:merge-rp} that setting $\thetaFuse$ to 0.9 leads to the best performance of  \mlm{}-CR  in terms of F1 score in comparison with other results obtained when adopting other similarity threshold values.
%As a consequence of this observation, we conclude that setting the similarity threshold  $\thetaFuse$ to 0.9 is suitable to ensure an acceptable quality of merge. 
%Accordingly, we decide in what follows to set $\thetaFuse = 0.9$ in all subsequent described experiments. 
As a consequence, we set the similarity threshold  $\thetaFuse$ to 0.9 in all subsequent described experiments. 


Our next experiment studies the impact of exploration sessions' size on the merge rates by varying the number of exploration steps per session. We report results for 30, 60, and 120 explorations steps per session. As before, we generate five random workloads of exploration sessions.
Each workload comprises around 1000 exploration steps distributed between exploration sessions of sizes equal to 30, 60, or 120. We study the average performance in terms of runtime and merge rate of our merge methods.
All results are presented in Figure~\ref{fig:condensate}, when setting $\thetaFuse = 0.9$. 


\begin{figure}[t]
\centering
\includegraphics[scale=0.5]{figures/merge/condensation_evaluation.pdf}
\caption{Exploration session size vs. merge rate (solid lines) and runtime (dashed lines)}
\label{fig:condensate}
\end{figure}


First, we observe that the runtime of \rlm{} remains constant over all exploration session sizes.
Associated with this runtime, we observe a constant drop of \rlm{} merge performance. This can be explained by the fact that the root nodes are consistently insufficiently matched, which entails that no matches can be found at deeper layers. As the exploration session's size increases, the number of layers increases as well, so the number of missed matching opportunities increases. 

In contrast to the \rlm{} performances, we see that the \mlm{} algorithms' runtimes increase roughly linearly despite their stable merge rates that are roughly constant over all exploration session sizes.
This linear behavior of \mlm{} 's runtime is related to the exploration sessions' sizes. 
Indeed, as the exploration session's size increases, the number of layers increases as well as and their sizes. Hence, this rise of the exploration session's size increases also the likelihood of obtaining large layers. This slows down the runtime of \mlm{} methods as we need to search possible matching in deep layers usually having large sizes.

Our final observation is that the merge rate of \mlm{}-CA method fluctuates, as it first increases to then significantly drop for exploration sessions of size 120. To understand this behavior, recall that in the case of conflicting matching candidates, \mlm{}-CA prioritizes the first discovered match, even if its distance is higher than the alternative matching candidate. Nonetheless, sub-graphs have most likely a big size. Thus, choosing a proposal matching with the lowest distance (that is done in \mlm{}-CR) will lead to a more efficient application of recursive merge between two big and highly similar sub-graphs.


To summarize, our evaluation of merge methods shows that we need to trade off efficiency with merge rate and merge quality. Higher merge rates and qualities are desirable to make more meaningful recommendations in the next step. Also, given that evolution provenance aggregation is performed offline, the runtime is not the main bottleneck. Given these results, we conclude that the \mlm{}-CR approach is the best suited in general 
%when setting $\thetaFuse = 0.9$, 
as it has comparable runtime to \mlm{}-CA while having more effective merge rates and better merges' quality.
